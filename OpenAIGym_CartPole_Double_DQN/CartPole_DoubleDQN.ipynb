{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Function Approximation for Q Learning\n",
    "\n",
    "Name: Aditya Raj Verma\n",
    " \n",
    "ID:   A53219148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cartpole\n",
    "\n",
    "A cartpole problem is shown below.\n",
    "![pendulum2.png](pendulum2.png)\n",
    "\n",
    "The equation for the cartpole problem is nonlinear in nature, but it has been shown through robust control theory that a linear version of the equation of the form $\\dot{x} = Ax+Bu$ can be solved by a linear controller. Let us assume that we are interested in minimizing cart stray from the center, and pendulum falling. It turns out that typical techniques - open loop control, PID control, root locus, etc. is not suitable for stabilizing both the cart position (keep near center) or the pole angle (keep vertical). The solution to this question is a linear quadratic controller, but we won't be using the solution at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment for Function Approximation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00941251,  0.00232819, -0.00728504, -0.01237385])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the CartPole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions possible: \t2\n",
      "State dimension: \t4\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "print \"Actions possible: \\t\", action_size\n",
    "print \"State dimension: \\t\", state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrate your understanding of the simulation\n",
    "For OpenAI's CartPole-v0 environment,\n",
    "- describe the reward system\n",
    "- describe the each state variable (observation space)\n",
    "- describe the action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Reward system: As long as the pole is maintained vertical(not fallen), the agent is rewarded 1 point. When the pole falls, agent is rewarded 0 and the game ends. <br>\n",
    "State variable: There are 4 state variables: <br>\n",
    "* Cart position <br>\n",
    "* Cart velocity<br>\n",
    "* Pole angle<br>\n",
    "* Pole tip velocity<br>\n",
    "\n",
    "\n",
    "Action space: Left or right movement of cart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Deep Neural Network class that creates a dense network of a desired architecture\n",
    "In this problem we will create neural network that is our function that takes states to q-values: $q=f(x)$. While any function approximator could be used (i.e. Chebyshev functions, taylor series polynomials), neural networks offer a most general form of 1st-order smooth function (though comprising of trivial small activation functions means that complex functions require a significant amount of weights to identify). \n",
    "\n",
    "Create a class for a QNetwork that uses PyTorch to create a fully connected sequential neural network, of the following properties:\n",
    "- solver: Adam\n",
    "\n",
    "- input and hidden layer activation function: tanh\n",
    "\n",
    "- output activation function: linear\n",
    "\n",
    "- loss: mse\n",
    "\n",
    "- learning_rate: variable\n",
    "\n",
    "- decay_rate: variable\n",
    "\n",
    "- hidden_state sizes: variable\n",
    "\n",
    "- state and action sizes: variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "# Define your network here   \n",
    "    def __init__(self, learning_rate, action_size = 2, state_size = 4, hidden_size = 64, alpha_decay = 0, scope = None ):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.scope = scope\n",
    "        self.alpha_decay = alpha_decay\n",
    "        with tf.variable_scope(scope):\n",
    "            self.build()\n",
    "            \n",
    "    def build(self, nHidden1 = 20):\n",
    "        # Input and output placeholder\n",
    "        self.inp = tf.placeholder(dtype= tf.float32, shape=(None, self.state_size), name=\"input\")\n",
    "        self.y_true = tf.placeholder(dtype = tf.float32, shape = (None, self.action_size), name = \"y_true\" )\n",
    "\n",
    "        # build model\n",
    "        with tf.variable_scope(\"cart-pole\"):\n",
    "            \n",
    "            # hidden layers\n",
    "            xx = tf.layers.dense(inputs = self.inp, units = self.hidden_size, activation = tf.nn.tanh , name=\"hidden_1\")\n",
    "            xx = tf.layers.dense(inputs = xx, units = self.hidden_size, activation = tf.nn.tanh , name=\"hidden_2\")\n",
    "            \n",
    "            # output layer\n",
    "            self.y_pred = tf.layers.dense(xx, self.action_size, name=\"y_pred\" )\n",
    "            #predicted output action\n",
    "            self.pred_action = tf.argmax(self.y_pred, 1, name = \"y_pred_action\")\n",
    "            \n",
    "            # calculate loss\n",
    "            self.loss = tf.losses.mean_squared_error(self.y_true, self.y_pred)\n",
    "\n",
    "            # optimization steps - with adaptive learning rate\n",
    "            self.global_step = tf.Variable(0, trainable=False)\n",
    "            self.starter_learning_rate = 0.001\n",
    "            self.learning_rate = tf.train.exponential_decay(self.starter_learning_rate, self.global_step,500, self.alpha_decay, staircase=True)\n",
    "            \n",
    "            # training op\n",
    "            self.train_op = tf.train.AdamOptimizer(learning_rate= self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "            \n",
    "    def train(self, inp, y_true):\n",
    "        feed_dict = {   self.inp : inp, self.y_true : y_true }             \n",
    "        [_, loss, _, _, abs_lr] = sess.run( [self.train_op, self.loss, self.y_pred, self.pred_action, self.learning_rate], feed_dict = feed_dict )\n",
    "        # save learning rate - useful to analyze adaptive learning rate\n",
    "        self.abs_lr = abs_lr\n",
    "\n",
    "    def predict(self, inp):\n",
    "        return sess.run( self.y_pred, feed_dict={self.inp:inp })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a Replay class that includes all the functionality of a replay buffer\n",
    "The replay buffer should kept to some maximum size (10000), allow adding of samples and returning of samples at random from the buffer. Each sample (or experience) is formed as (state, action, reward, next_state, done). The replay buffer should also be able to generate a minibatch. The generate_minibatch method should take in DQN, targetDQN, selected batch_size, and return the states present in the minibatch and the target Q values for those states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Replay():\n",
    "# Replay should also have an initialize method which creates a minimum buffer for \n",
    "# the initial episodes to generate minibatches. \n",
    "    def __init__(self, dqn, targetDqn,  max_size = 10000, gamma = 0.95):\n",
    "        self.memory_size = max_size\n",
    "        self.memory = []\n",
    "        self.batch_size = 32\n",
    "        self.dqn = dqn\n",
    "        self.targetDqn = targetDqn\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 0.5\n",
    "        self.replace_index = 0\n",
    "        \n",
    "    def initialize(self, init_length=1000, envir=env):\n",
    "        state = env.reset().reshape([1,4])\n",
    "        for _ in range(init_length):\n",
    "            action = np.random.randint(2) #self.getAction(state)\n",
    "            next_state, reward, done , _ = env.step(action)\n",
    "            next_state = next_state.reshape([-1,4])\n",
    "\n",
    "            self.remember(state, action, reward, next_state, done)\n",
    "            if done:\n",
    "                state = env.reset().reshape([1,4])\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) < self.memory_size:\n",
    "            self.memory.append([state, action, reward, next_state, done])\n",
    "        else:\n",
    "            self.memory[self.replace_index] = [state, action, reward, next_state, done]\n",
    "            self.replace_index += 1\n",
    "            if self.replace_index == self.memory_size:\n",
    "                self.replace_index = 0\n",
    "        \n",
    "    def generate_minibatch(self, batch_size=None):\n",
    "        batch_size = batch_size if batch_size else self.batch_size\n",
    "        batch_size = min( batch_size, len(self.memory))\n",
    "        minibatch = random.sample(self.memory, batch_size)    \n",
    "        return minibatch\n",
    "            \n",
    "    def getAction(self, state, epsilon, dqn ):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(action_size)\n",
    "        else:\n",
    "            return np.argmax( self.dqn.predict(state)[0] )\n",
    "        \n",
    "    def copyWeights(self, scope_src='dqn', scope_dest='targetDqn'):\n",
    "        varList_src  =  tf.trainable_variables(scope= scope_src)\n",
    "        varList_dest =  tf.trainable_variables(scope= scope_dest)\n",
    "\n",
    "        for v_src, v_dest in zip(varList_src, varList_dest):\n",
    "            temp_op = tf.assign(v_dest, v_src)\n",
    "            sess.run(temp_op)\n",
    "        \n",
    "    def trainOnHistoryBatch(self, batch_size = None):\n",
    "        #  generate minibatch from historical evidences\n",
    "        minibatch = self.generate_minibatch(batch_size)\n",
    "        train_x = np.zeros((len(minibatch), state_size))\n",
    "        train_y = np.zeros((len(minibatch), action_size))\n",
    "        for i, [state, action, reward, next_state, done] in enumerate(minibatch):\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma*( np.max(self.targetDqn.predict(next_state)[0]))\n",
    "            \n",
    "            target_f = self.dqn.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            train_x[i] = state\n",
    "            train_y[i] = target_f[0]\n",
    "#         return train_x, train_y\n",
    "        # update DQN (run one epoch of training per episode with generated minibatch of states and qvalues)\n",
    "        self.dqn.train(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that creates a minibatch from a buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Function Approximation\n",
    "Initialize DQN networks and Replay objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with your learning rate, alpha decay and hidden layer units \n",
    "# Two layers with a small number of units should be enough\n",
    "tf.reset_default_graph()\n",
    "# if sess:\n",
    "#     sess.close()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize DQN\n",
    "DQN = QNetwork(learning_rate = 0.001, state_size = 4, action_size = 2, hidden_size = 128, alpha_decay = 0.6, scope = 'dqn')\n",
    "targetDQN = QNetwork(learning_rate = 0.001, state_size = 4, action_size = 2, hidden_size = 128, alpha_decay = 0.5, scope = 'targetDqn')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "file_writer = tf.summary.FileWriter('/home/aditya/UCSD/Spring18/ECE276C_RoboticReinforcementLearning/HW2/logs')\n",
    "file_writer.add_graph(sess.graph)\n",
    "# set targetDQN weights to DQN weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function that solves the above environment using a deep Q network that uses a minibatch strategy.\n",
    "Use the following parameters (these had to be derived empirically - there is generally no trusted way of choosing the right parameter values - i.e. gamma, number of episodes, decay rate, min_epsilon). \n",
    "\n",
    "Generate a graph of the average return per episode every 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  50 \t\t Return:  34 \t\t Lr =  0.001\n",
      "Episode:  100 \t\t Return:  12 \t\t Lr =  0.001\n",
      "Episode:  150 \t\t Return:  14 \t\t Lr =  0.001\n",
      "Episode:  200 \t\t Return:  33 \t\t Lr =  0.001\n",
      "Episode:  250 \t\t Return:  42 \t\t Lr =  0.001\n",
      "Episode:  300 \t\t Return:  31 \t\t Lr =  0.001\n",
      "Episode:  350 \t\t Return:  27 \t\t Lr =  0.001\n",
      "Episode:  400 \t\t Return:  104 \t\t Lr =  0.001\n",
      "Episode:  450 \t\t Return:  200 \t\t Lr =  0.0006\n",
      "Episode:  500 \t\t Return:  200 \t\t Lr =  0.0006\n",
      "Episode:  550 \t\t Return:  200 \t\t Lr =  0.0006\n",
      "Episode:  600 \t\t Return:  196 \t\t Lr =  0.0006\n",
      "Episode:  650 \t\t Return:  200 \t\t Lr =  0.0006\n",
      "Episode:  700 \t\t Return:  149 \t\t Lr =  0.0006\n",
      "Episode:  750 \t\t Return:  200 \t\t Lr =  0.0006\n",
      "Episode:  800 \t\t Return:  200 \t\t Lr =  0.0006\n",
      "Episode:  850 \t\t Return:  200 \t\t Lr =  0.0006\n",
      "Episode:  900 \t\t Return:  200 \t\t Lr =  0.0006\n",
      "Episode:  950 \t\t Return:  200 \t\t Lr =  0.00036000003\n",
      "Episode:  1000 \t\t Return:  200 \t\t Lr =  0.00036000003\n",
      "Episode:  1050 \t\t Return:  200 \t\t Lr =  0.00036000003\n",
      "Episode:  1100 \t\t Return:  176 \t\t Lr =  0.00036000003\n",
      "Episode:  1150 \t\t Return:  200 \t\t Lr =  0.00036000003\n",
      "Episode:  1200 \t\t Return:  200 \t\t Lr =  0.00036000003\n",
      "Episode:  1250 \t\t Return:  200 \t\t Lr =  0.00036000003\n",
      "Episode:  1300 \t\t Return:  200 \t\t Lr =  0.00036000003\n",
      "Episode:  1350 \t\t Return:  162 \t\t Lr =  0.00036000003\n",
      "Episode:  1400 \t\t Return:  200 \t\t Lr =  0.00036000003\n",
      "Episode:  1450 \t\t Return:  186 \t\t Lr =  0.00021600003\n",
      "Episode:  1500 \t\t Return:  157 \t\t Lr =  0.00021600003\n",
      "Episode:  1550 \t\t Return:  176 \t\t Lr =  0.00021600003\n",
      "Episode:  1600 \t\t Return:  109 \t\t Lr =  0.00021600003\n",
      "Episode:  1650 \t\t Return:  200 \t\t Lr =  0.00021600003\n",
      "Episode:  1700 \t\t Return:  200 \t\t Lr =  0.00021600003\n",
      "Episode:  1750 \t\t Return:  200 \t\t Lr =  0.00021600003\n",
      "Episode:  1800 \t\t Return:  200 \t\t Lr =  0.00021600003\n",
      "Episode:  1850 \t\t Return:  200 \t\t Lr =  0.00021600003\n",
      "Episode:  1900 \t\t Return:  200 \t\t Lr =  0.00021600003\n",
      "Episode:  1950 \t\t Return:  175 \t\t Lr =  0.00012960003\n"
     ]
    }
   ],
   "source": [
    "# Runtime parameters\n",
    "num_episodes = 2000            # max number of episodes to learn from\n",
    "gamma = 0.95                  # future reward discount\n",
    "max_steps = 500                # cut off simulation after this many steps\n",
    "\n",
    "# Exploration parameters\n",
    "min_epsilon = 0.01             # minimum exploration probability\n",
    "decay_rate = 5.0/num_episodes    # exponential decay rate for exploration prob\n",
    "returns = np.zeros(num_episodes)\n",
    "updateFreq = 50\n",
    "\n",
    "\n",
    "## Initialize Replay Buffer\n",
    "###################################\n",
    "## Populate the initial experience buffer\n",
    "###################################\n",
    "replay = Replay(dqn = DQN, targetDqn = targetDQN, max_size=10000, gamma = gamma)\n",
    "replay.initialize(init_length=1000, envir=env)\n",
    "\n",
    "\n",
    "# --> start episode \n",
    "for ep in range(1, num_episodes):\n",
    "    state = env.reset().reshape([-1,state_size])\n",
    "    epsilon = min_epsilon + (1.0 - min_epsilon)*np.exp(-decay_rate*ep)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # explore/exploit and get action using DQN\n",
    "        action = replay.getAction(state, epsilon, DQN)\n",
    "        # perform action and record new_state, action, reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = next_state.reshape([-1,state_size])\n",
    "        # populate Replay experience buffer\n",
    "        replay.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        returns[ep] += reward\n",
    "        state = next_state\n",
    "        # <-- end episode\n",
    "\n",
    "    # set targetDQN weights to DQN weights\n",
    "    if ep%updateFreq == 0:\n",
    "        replay.copyWeights('dqn','targetDqn')\n",
    "        print \"Episode: \", ep, \"\\t\\t Return: \", int(returns[ep]), \"\\t\\t Lr = \", DQN.abs_lr\n",
    "\n",
    "    # Replay\n",
    "    replay.trainOnHistoryBatch(256)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effad205610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot average returns\n",
    "returns_over_100_episodes = []\n",
    "x = []\n",
    "for i in range(0,int(num_episodes/100)):\n",
    "    returns_over_100_episodes.append(sum(returns[100*i:100*(i+1)-1])/100)\n",
    "    x.append((i+1)*100)\n",
    "plt.plot(x,returns_over_100_episodes,'.-r')\n",
    "plt.ylabel('Average Returns per Episode')\n",
    "plt.xlabel('Num of Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABBlJREFUeJzt3O1pwlAAhtGkuETn6BqdQ9foGnWO\nrtE5Okb6x36ACIq01zyeA4IIygvCwyUY52VZJgB6HkYPAOBvCDxAlMADRAk8QJTAA0QJPECUwANE\nCTxAlMADRG1GDzhwOy3AsfmaNzvBA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMAD\nRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANE\nCTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJ\nPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8\nQJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxA\nlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECU\nwANEbUYPAG7b+3539NrT9nXAEi7lBA8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFEC\nDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIP\nECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg+c9L7fjZ7AFQQeIErgAaIEHiBK4AGi\nBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIE\nHiBK4AGiBB4gSuDhzszzfPbj2s9gLIEHiNqMHgDctreP7ffz58f9wCVcSuCBs/3EXujXwCUa4KTf\np3fWR+CBk1ySWTeXaICzfQX/ZewMzjQvyzJ6wzRN002MgHvwnz9fvJG+rNlVX5ZLNABRAg8QJfAA\nUQIPECXwAFECDxAl8ABRAg8QJfAAUf6qAO6Mu0vvhxM8QJTAA0QJPECUwANECTxAlMADRAk8QJTA\nA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMAD\nRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANE\nCTxAlMADRAk8QJTAA0QJPEDUZvSAg3n0AIAaJ3iAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4\ngCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeICoTynJHyQO\npH2kAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effaf7efe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABBlJREFUeJzt3O1pwlAAhtGkuETn6BqdQ9foGnWO\nrtE5Okb6x36ACIq01zyeA4IIygvCwyUY52VZJgB6HkYPAOBvCDxAlMADRAk8QJTAA0QJPECUwANE\nCTxAlMADRG1GDzhwOy3AsfmaNzvBA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMAD\nRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANE\nCTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJ\nPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8\nQJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxA\nlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECU\nwANEbUYPAG7b+3539NrT9nXAEi7lBA8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFEC\nDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg8QJfAAUQIP\nECXwAFECDxAl8ABRAg8QJfAAUQIPECXwAFECDxAl8ABRAg+c9L7fjZ7AFQQeIErgAaIEHiBK4AGi\nBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIEHiBK4AGiBB4gSuABogQeIErgAaIE\nHiBK4AGiBB4gSuDhzszzfPbj2s9gLIEHiNqMHgDctreP7ffz58f9wCVcSuCBs/3EXujXwCUa4KTf\np3fWR+CBk1ySWTeXaICzfQX/ZewMzjQvyzJ6wzRN002MgHvwnz9fvJG+rNlVX5ZLNABRAg8QJfAA\nUQIPECXwAFECDxAl8ABRAg8QJfAAUf6qAO6Mu0vvhxM8QJTAA0QJPECUwANECTxAlMADRAk8QJTA\nA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMAD\nRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANECTxAlMADRAk8QJTAA0QJPECUwANE\nCTxAlMADRAk8QJTAA0QJPEDUZvSAg3n0AIAaJ3iAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4\ngCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeIAogQeIEniAKIEHiBJ4gCiBB4gSeICoTynJHyQO\npH2kAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7effaf7efe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# DEMO FINAL NETWORK\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "state = np.reshape(state, [1, state.size])\n",
    "total_reward = 0\n",
    "for i in range(0, max_steps):\n",
    "    #env.render()\n",
    "    show_state(env,i)\n",
    "    \n",
    "    Qs = DQN.predict(state)\n",
    "    # Get action from Q-network\n",
    "    # Qs = output of DQN.model when state is passed in\n",
    "    action = np.argmax(Qs)\n",
    "    \n",
    "    # Take action, get new state and reward\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    if done:\n",
    "        #env.close()\n",
    "        break\n",
    "    else:\n",
    "        state = np.reshape(next_state, [1, state.size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
