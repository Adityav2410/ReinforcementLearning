{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Policy Gradients (DDPG and REINFORCE)\n",
    "\n",
    "Name: Aditya Raj Verma\n",
    " \n",
    "ID:   A53219148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "This exercise requires you to solve various continous control problems in OpenAI-Gym.  \n",
    "\n",
    "DDPG is policy gradient actor critic method for continous control which is off policy. It tackles the curse of dimensionality / loss of performance faced when discretizing a continous action domain. DDPG uses similiar \"tricks\" as DQN to improve the stability of training, including a replay buffer and target networks.\n",
    "\n",
    "Furthermore, you will implement REINFORCE for discrete and continous environments, and as a bonus compare the sample efficiency and performance with DQN and DDPG.\n",
    "\n",
    "\n",
    "### DDPG paper: https://arxiv.org/pdf/1509.02971.pdf\n",
    "\n",
    "### Environments:\n",
    "\n",
    "#### InvertedPendulum-v2 environment:\n",
    "<img src=\"inverted_pendulum.png\" width=\"300\">\n",
    "\n",
    "#### Pendulum-v0 environment:\n",
    "<img src=\"pendulum.png\" width=\"300\">\n",
    "\n",
    "#### Halfcheetah-v2 environment:\n",
    "<img src=\"half_cheetah.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment for Actor Critic\n",
    "- inline plotting\n",
    "- gym\n",
    "- directory for logging videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#environment\n",
    "import gym\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "logging_interval = 100\n",
    "animate_interval = logging_interval * 5\n",
    "logdir='./DDPG/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up gym environment\n",
    "The code below does the following for you:\n",
    "- Wrap environment, log videos, setup CUDA variables (if GPU is available)\n",
    "- Record action and observation space dimensions\n",
    "- Fix random seed for determinisitic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "VISUALIZE = False\n",
    "SEED = 0\n",
    "MAX_PATH_LENGTH = 500\n",
    "NUM_EPISODES = 12000\n",
    "GAMMA=0.99\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Environments to be tested on\n",
    "# env_name = 'InvertedPendulum-v2'\n",
    "env_name = 'Pendulum-v0'\n",
    "#env_name = 'HalfCheetah-v2' \n",
    "\n",
    "# wrap gym to save videos\n",
    "env = gym.make(env_name)\n",
    "if VISUALIZE:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = gym.wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "env._max_episodes_steps = MAX_PATH_LENGTH\n",
    "\n",
    "# check observation and action space\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    print(\"This is a discrete action space, probably not the right algorithm to use\")\n",
    "\n",
    "# set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# make variable types for automatic setting to GPU or CPU, depending on GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "def to_numpy(var):\n",
    "    return var.cpu().data.numpy() if use_cuda else var.data.numpy()\n",
    "\n",
    "def to_tensor(x, volatile=False, requires_grad=True, dtype=Tensor):\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = Variable(x, requires_grad=requires_grad).type(dtype)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate your understanding of the simulation:\n",
    "For the environments mentioned above ('Pendulum-v0', 'HalfCheetah-v2', 'InvertedPendulum-v2'),\n",
    "- describe the reward system\n",
    "- describe the each state variable (observation space)\n",
    "- describe the action space\n",
    "- when is the environment considered \"solved\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an action normalization class:\n",
    "To train across various environments, it is useful to normalize action inputs and outputs between [-1, 1]. This class should take in actions and implement forward and reverse functions to map actions between [-1, 1] and [action_space.low, action_space.high].\n",
    "\n",
    "Using the following gym wrapper, implement this class.\n",
    "- https://github.com/openai/gym/blob/78c416ef7bc829ce55b404b6604641ba0cf47d10/gym/core.py\n",
    "- i.e. we are overriding the outputs scale of actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeAction(gym.ActionWrapper):\n",
    "    def _reverse_action(self, action):\n",
    "        #tanh outputs (-1,1) from tanh, need to be [action_space.low, action_space.high]\n",
    "        self.low = self.action_space.low\n",
    "        self.high = self.action_space.high\n",
    "        self.mid = self.low + (self.high - self.low)/2\n",
    "        self.range = (self.high - self.low)/2.\n",
    "        return (action-self.mid)/self.range \n",
    "\n",
    "    def _action(self, action):\n",
    "        #reverse of that above\n",
    "        self.low = self.action_space.low\n",
    "        self.high = self.action_space.high\n",
    "        self.mid = self.low + (self.high - self.low)/2\n",
    "        self.range = (self.high - self.low)/2.\n",
    "        return action * self.range + self.mid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env = NormalizeAction(env)\n",
    "dim_state = env.observation_space.shape[0]\n",
    "dim_action = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a weight syncing function\n",
    "In contrast to DQN, DDPG uses soft weight sychronization. At each time step following training, the actor and critic target network weights are updated to track the rollout networks. \n",
    "- target_network.weights <= target_network.weights \\* (1 - tau) + source_network.weights \\* (tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightSync(target_model, source_model, tau = 0.001):\n",
    "    for parameter_target, parameter_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        parameter_target.data.copy_((1 - tau) * parameter_target.data + tau * parameter_source.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Replay class that includes all the functionality of a replay buffer\n",
    "DDPG is an off policy actor-critic method and an identical replay buffer to that used for the previous assignment is applicable here as well (do not include the generate_minibatch method in your Replay class this time). Like before, your constructor for Replay should create an initial buffer of size 1000 when you instantiate it.\n",
    "\n",
    "The replay buffer should kept to some maximum size (60000), allow adding of samples and returning of samples at random from the buffer. Each sample (or experience) is formed as (state, action, reward, next_state, done). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Replay:\n",
    "    def __init__(self, env, batch_size = 128):\n",
    "        \n",
    "        self.capacity = 60000 \n",
    "        #  For each experience - 5 information will be saved \n",
    "        self.memory = []   #[state, action, reward, next_state, done]\n",
    "        self.curr_index = 0\n",
    "        self.size = 0\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Setup\n",
    "        self.env = env\n",
    "        \n",
    "        # Call function to add initial 1000 experiences so that training can start\n",
    "        self.initializeInitialExperience()\n",
    "        \n",
    "    def initializeInitialExperience(self):\n",
    "        state = self.env.reset()\n",
    "        for i in range(1000):\n",
    "            # Need to Add 1000 experience\n",
    "            action = np.random.uniform(low = -1.0, high = 1.0, size = self.env.action_space.shape[0] )\n",
    "            next_state, reward, done , _ = self.env.step(action)\n",
    "            \n",
    "            self.add_experience(state, action, reward, next_state, done)\n",
    "            self.curr_index += 1\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "        \n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        if self.curr_index >= self.capacity:\n",
    "            self.curr_index = 0\n",
    "        curr_experience = defaultdict()\n",
    "        curr_experience['state'] = state\n",
    "        curr_experience['action'] = action\n",
    "        curr_experience['reward'] = reward\n",
    "        curr_experience['next_state'] = next_state\n",
    "        curr_experience['done'] = done\n",
    "        \n",
    "        if self.size < self.capacity:\n",
    "            self.memory.append(curr_experience)\n",
    "        else:\n",
    "            self.memory[self.curr_index] = curr_experience\n",
    "        self.curr_index += 1\n",
    "        self.size = min(self.size + 1, self.capacity )\n",
    "        \n",
    "    def sample(self, batch_size = None ):\n",
    "        batch_size = batch_size if batch_size else self.batch_size\n",
    "\n",
    "        # Prepare batch-data holder \n",
    "        batch_state = np.zeros(shape=(batch_size, dim_state))\n",
    "        batch_action = np.zeros(shape=(batch_size, dim_action))\n",
    "        batch_reward = np.zeros(shape=(batch_size,1))\n",
    "        batch_next_state = np.zeros(shape=(batch_size, dim_state))\n",
    "        batch_done = np.zeros(shape=(batch_size,1))\n",
    "\n",
    "        \n",
    "        # Sample mini-batch\n",
    "        sampled_experiences = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Copy sampled minibatch to numpy array\n",
    "        for i,curr_experience in enumerate(sampled_experiences):\n",
    "            batch_state[i] = curr_experience['state']\n",
    "            batch_action[i] = curr_experience['action']\n",
    "            batch_reward[i] = curr_experience['reward'].reshape((-1,1))\n",
    "            batch_next_state[i] = curr_experience['next_state']\n",
    "            batch_done[i] = curr_experience['done']\n",
    "            \n",
    "        return batch_state, batch_action, batch_reward, batch_next_state, batch_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write an Ornstein Uhlenbeck process class for exploration noise\n",
    "The proccess is described here:\n",
    "- https://en.wikipedia.org/wiki/Ornstein–Uhlenbeck_process\n",
    "- http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "\n",
    "You should implement:\n",
    "- a step / sample method\n",
    "- reset method\n",
    "\n",
    "Use theta = 0.15, mu = 0, sigma = 0.3, dt = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckProcess(object):\n",
    "    def __init__(self, dimension, num_steps, theta=0.25, mu=0.0, sigma=0.05, dt=0.01):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x = np.zeros((dimension,))\n",
    "        self.iter = 0\n",
    "        self.num_steps = num_steps\n",
    "        self.dimension = dimension\n",
    "        self.min_epsilon = 0.01 # minimum exploration probability\n",
    "        self.epsilon = 1.0\n",
    "        self.decay_rate = 5.0/num_steps # exponential decay rate for exploration prob\n",
    "    \n",
    "    def sample(self):\n",
    "        self.x = self.x + self.theta*(self.mu-self.x)*self.dt + \\\n",
    "                                       self.sigma*np.sqrt(self.dt)*np.random.normal(size=self.dimension)\n",
    "        return self.epsilon*self.x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x = 0*self.x\n",
    "        self.iter += 1\n",
    "        self.epsilon = self.min_epsilon + (1.0 - self.min_epsilon)*np.exp(-self.decay_rate*self.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise =  OrnsteinUhlenbeckProcess(dimension=1,num_steps=1000)\n",
    "y = np.zeros((1000))\n",
    "for i in range(1000):\n",
    "    y[i] = noise.sample()\n",
    "plt.plot(range(1000),y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Deep Neural Network class that creates a dense network of a desired architecture for actor and critic networks\n",
    "\n",
    "\n",
    "#### Actor\n",
    "- input and hidden layer activation function: ReLU\n",
    "\n",
    "- output activation function: Tanh\n",
    "\n",
    "- hidden_state sizes: 400\n",
    "\n",
    "- state and action sizes: variable\n",
    "\n",
    "- number of hidden layers: 2\n",
    "\n",
    "- batch normalization applied to all hidden layers\n",
    "\n",
    "- weight initialization: normal distribution with small variance. \n",
    "\n",
    "#### Critic\n",
    "- input and hidden layer activation function: ReLU\n",
    "\n",
    "- output activation function: None\n",
    "\n",
    "- hidden_state sizes: 300, 300 + action size\n",
    "\n",
    "- state and action sizes: variable\n",
    "\n",
    "- number of hidden layers: 2\n",
    "\n",
    "- batch normalization applied to all hidden layers prior to the action input\n",
    "\n",
    "- weight initialization: normal distribution with small variance.\n",
    "\n",
    "Good baselines can be found in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "# ----------------------------------------------------\n",
    "# actor model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 2 hidden layers, 400 units per layer, tanh output to bound outputs between -1 and 1\n",
    "\n",
    "class actor(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(actor, self).__init__()\n",
    "        \n",
    "        # Input/Output shapes\n",
    "        self.dim_state = input_size\n",
    "        self.dim_action = output_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.dim_state , 400 )\n",
    "        self.bn1 = nn.BatchNorm1d(400)\n",
    "        self.fc2 = nn.Linear(400, 400 )\n",
    "        self.bn2 = nn.BatchNorm1d(400)\n",
    "        self.fc3 = nn.Linear(400, self.dim_action)\n",
    "        \n",
    "        # Xavier initialization of layers\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # Pass state through FC layer\n",
    "        out = self.fc1(state)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        # Concatenate fc_state and action \n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out)\n",
    "        predicted_action = F.tanh( self.fc3(out))\n",
    "        return predicted_action\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# critic model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 2 hidden layers, 300 units per layer, ouputs rewards therefore unbounded\n",
    "# Action not to be included until 2nd layer of critic (from paper). Make sure to formulate your critic.forward() accordingly\n",
    "\n",
    "class critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, output_size = 1):\n",
    "        super(critic, self).__init__()\n",
    "        \n",
    "        # Input/Output shapes\n",
    "        self.dim_state = state_size\n",
    "        self.dim_action = action_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.dim_state , 300 )\n",
    "        self.fc2 = nn.Linear(300 + self.dim_action, 300 )\n",
    "        self.fc3 = nn.Linear(300, output_size)\n",
    "        \n",
    "        # Xavier initialization of layers\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        # Pass state through FC layer\n",
    "        out = F.relu( self.fc1(state) )\n",
    "        # Concatenate fc_state and action \n",
    "        out = F.relu(  self.fc2(torch.cat([out, action], 1))      )\n",
    "        q_value = self.fc3(out)\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DDPG class to encapsulate definition, rollouts, and training\n",
    "\n",
    "- gamma = 0.99\n",
    "\n",
    "- actor_lr = 1e-4\n",
    "\n",
    "- critic_lr = 1e-3\n",
    "\n",
    "- critic l2 regularization = 1e-2\n",
    "\n",
    "- noise decay\n",
    "\n",
    "- noise class\n",
    "\n",
    "- batch_size = 128\n",
    "\n",
    "- optimizer: Adam\n",
    "\n",
    "- loss (critic): mse\n",
    "\n",
    "Furthermore, you can experiment with action versus parameter space noise. The standard implimentation works with action space noise, howeve parameter space noise has shown to produce excellent results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace as bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_DIM = env.observation_space.shape[0]\n",
    "ACT_DIM = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, obs_dim, act_dim, critic_lr = 1e-3, actor_lr = 1e-4, gamma = GAMMA, batch_size = BATCH_SIZE):\n",
    "\n",
    "        alpha_decay = 0.93\n",
    "        self.gamma = GAMMA\n",
    "        self.batch_size = BATCH_SIZE\n",
    "\n",
    "        # actor\n",
    "        self.actor = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor.cuda()\n",
    "        self.actor_target = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_target.cuda()\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        # critic\n",
    "        self.critic = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic_target = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic.cuda()\n",
    "        self.critic_target.cuda()\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # optimizers\n",
    "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr = actor_lr)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr = critic_lr, weight_decay=1e-2)\n",
    "\n",
    "        # learning rate scheduler\n",
    "        self.scheduler_actor = optim.lr_scheduler.StepLR(self.optimizer_actor, step_size=5000, gamma=alpha_decay)\n",
    "        self.scheduler_critic = optim.lr_scheduler.StepLR(self.optimizer_critic, step_size=5000, gamma=alpha_decay)\n",
    "        \n",
    "        # critic loss\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "        \n",
    "        # noise\n",
    "        self.noise = OrnsteinUhlenbeckProcess(dimension = act_dim, num_steps = NUM_EPISODES)\n",
    "\n",
    "        # replay buffer \n",
    "        self.replayBuffer = Replay(env)\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        # sample from Replay\n",
    "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = self.replayBuffer.sample(self.batch_size)\n",
    "\n",
    "        # update critic (create target for Q function)\n",
    "        target_q = self.critic_target( to_tensor(batch_next_state, volatile = True),\\\n",
    "                                       self.actor_target(to_tensor(batch_next_state,volatile = True)) )\n",
    "        true_q = to_numpy( to_tensor(batch_reward) + self.gamma * target_q * to_tensor(1-batch_done) )\n",
    "\n",
    "        predicted_q = self.critic(to_tensor(batch_state), to_tensor(batch_action))\n",
    "        critic_loss = self.critic_loss( predicted_q,  to_tensor(true_q, requires_grad = False) )\n",
    "\n",
    "       \n",
    "        # critic optimizer and backprop step (feed in target and predicted values to self.critic_loss)\n",
    "        self.critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        self.scheduler_critic.step()\n",
    "\n",
    "#         # update actor (formulate the loss wrt which actor is updated)\n",
    "        policy_loss = -self.critic(to_tensor(batch_state), self.actor(to_tensor(batch_state)))\n",
    "        policy_loss = policy_loss.mean()\n",
    "#         # actor optimizer and backprop step (loss_actor.backward())\n",
    "        self.actor.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        self.scheduler_actor.step()\n",
    "\n",
    "#         # sychronize target network with fast moving one\n",
    "        weightSync(self.critic_target, self.critic)\n",
    "        weightSync(self.actor_target, self.actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of your DDPG object\n",
    "- Print network architectures, confirm they are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor(\n",
      "  (fc1): Linear(in_features=3, out_features=400, bias=True)\n",
      "  (bn1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (bn2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc3): Linear(in_features=400, out_features=1, bias=True)\n",
      ")\n",
      "critic(\n",
      "  (fc1): Linear(in_features=3, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=301, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ddpg = DDPG(obs_dim = dim_state, act_dim = dim_action)\n",
    "print(ddpg.actor)\n",
    "print(ddpg.critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DDPG on different environments\n",
    "Early stopping conditions:\n",
    "- avg_val > 500 for \"InvertedPendulum\" \n",
    "- avg_val > -150 for \"Pendulum\" \n",
    "- avg_val > 1500 for \"HalfCheetah\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 -1341.2721850292692\n",
      "Average value: -67.06360925146346 for episode: 0\n",
      "400 -1477.1529868276878\n",
      "Average value: -137.56807813027467 for episode: 1\n",
      "600 -1583.952216391029\n",
      "Average value: -209.88728504331237 for episode: 2\n",
      "800 -1437.0582631055781\n",
      "Average value: -271.24583394642565 for episode: 3\n",
      "1000 -1409.7933954183197\n",
      "Average value: -328.17321202002034 for episode: 4\n",
      "1200 -1507.3720746285403\n",
      "Average value: -387.1331551504463 for episode: 5\n",
      "1400 -1510.698249565345\n",
      "Average value: -443.31140987119124 for episode: 6\n",
      "1600 -1521.537884449595\n",
      "Average value: -497.2227336001114 for episode: 7\n",
      "1800 -1412.6491331125048\n",
      "Average value: -542.9940535757311 for episode: 8\n",
      "2000 -1180.7075106615273\n",
      "Average value: -574.8797264300209 for episode: 9\n",
      "2200 -1479.6183793799828\n",
      "Average value: -620.116659077519 for episode: 10\n",
      "2400 -1484.4203784455938\n",
      "Average value: -663.3318450459227 for episode: 11\n",
      "2600 -1427.6350626606622\n",
      "Average value: -701.5470059266597 for episode: 12\n",
      "2800 -1453.0812756357307\n",
      "Average value: -739.1237194121131 for episode: 13\n",
      "3000 -1532.2006230858942\n",
      "Average value: -778.7775645958021 for episode: 14\n",
      "3200 -1429.1289524705985\n",
      "Average value: -811.2951339895419 for episode: 15\n",
      "3400 -1463.9423463122139\n",
      "Average value: -843.9274946056755 for episode: 16\n",
      "3600 -1421.640583568664\n",
      "Average value: -872.813149053825 for episode: 17\n",
      "3800 -7.387003806600585\n",
      "Average value: -829.5418417914638 for episode: 18\n",
      "4000 -13.182104039047799\n",
      "Average value: -788.7238549038428 for episode: 19\n",
      "4200 -1439.974769619586\n",
      "Average value: -821.28640063963 for episode: 20\n",
      "4400 -1461.924248129943\n",
      "Average value: -853.3182930141455 for episode: 21\n",
      "4600 -1447.801877730195\n",
      "Average value: -883.0424722499479 for episode: 22\n",
      "4800 -1497.0499777585271\n",
      "Average value: -913.7428475253768 for episode: 23\n",
      "5000 -1395.748700902916\n",
      "Average value: -937.8431401942537 for episode: 24\n",
      "5200 -1392.2313449601052\n",
      "Average value: -960.5625504325462 for episode: 25\n",
      "5400 -1493.4839172975937\n",
      "Average value: -987.2086187757986 for episode: 26\n",
      "5600 -1407.618033185105\n",
      "Average value: -1008.229089496264 for episode: 27\n",
      "5800 -1391.8742565111536\n",
      "Average value: -1027.4113478470085 for episode: 28\n",
      "6000 -1367.856902064121\n",
      "Average value: -1044.4336255578642 for episode: 29\n",
      "6200 -1370.397979035262\n",
      "Average value: -1060.731843231734 for episode: 30\n",
      "6400 -1430.0896042876282\n",
      "Average value: -1079.1997312845288 for episode: 31\n",
      "6600 -1498.1231138785454\n",
      "Average value: -1100.1459004142296 for episode: 32\n",
      "6800 -1347.0426682320058\n",
      "Average value: -1112.4907388051183 for episode: 33\n",
      "7000 -1233.735779657222\n",
      "Average value: -1118.5529908477235 for episode: 34\n",
      "7200 -549.4385180524935\n",
      "Average value: -1090.097267207962 for episode: 35\n",
      "7400 -918.081694722795\n",
      "Average value: -1081.4964885837037 for episode: 36\n",
      "7600 -681.8452630051981\n",
      "Average value: -1061.5139273047785 for episode: 37\n",
      "7800 -771.2229406788425\n",
      "Average value: -1046.9993779734816 for episode: 38\n",
      "8000 -273.2005421465481\n",
      "Average value: -1008.309436182135 for episode: 39\n",
      "8200 -407.592616844962\n",
      "Average value: -978.2735952152763 for episode: 40\n",
      "8400 -1063.3495353326748\n",
      "Average value: -982.5273922211461 for episode: 41\n",
      "8600 -580.9818134349409\n",
      "Average value: -962.4501132818358 for episode: 42\n",
      "8800 -1498.8537980227798\n",
      "Average value: -989.270297518883 for episode: 43\n",
      "9000 -1059.8182094644542\n",
      "Average value: -992.7976931161616 for episode: 44\n",
      "9200 -139.93007095714734\n",
      "Average value: -950.1543120082108 for episode: 45\n",
      "9400 -274.26320013277524\n",
      "Average value: -916.359756414439 for episode: 46\n",
      "9600 -583.0754583028786\n",
      "Average value: -899.6955415088609 for episode: 47\n",
      "9800 -576.4150589194413\n",
      "Average value: -883.5315173793899 for episode: 48\n",
      "10000 -595.8408820218826\n",
      "Average value: -869.1469856115145 for episode: 49\n",
      "10200 -268.45749096468177\n",
      "Average value: -839.1125108791729 for episode: 50\n",
      "10400 -277.76620604158273\n",
      "Average value: -811.0451956372933 for episode: 51\n",
      "10600 -269.11123399115047\n",
      "Average value: -783.9484975549861 for episode: 52\n",
      "10800 -280.69703220350794\n",
      "Average value: -758.7859242874123 for episode: 53\n",
      "11000 -140.25740819241614\n",
      "Average value: -727.8594984826625 for episode: 54\n",
      "11200 -527.5761221418932\n",
      "Average value: -717.845329665624 for episode: 55\n",
      "11400 -136.18051044175752\n",
      "Average value: -688.7620887044307 for episode: 56\n",
      "11600 -5.14213065709736\n",
      "Average value: -654.581090802064 for episode: 57\n",
      "11800 -568.6630200490106\n",
      "Average value: -650.2851872644112 for episode: 58\n",
      "12000 -263.59919040983993\n",
      "Average value: -630.9508874216826 for episode: 59\n",
      "12200 -714.5568019965075\n",
      "Average value: -635.1311831504238 for episode: 60\n",
      "12400 -129.09697702135057\n",
      "Average value: -609.82947284397 for episode: 61\n",
      "12600 -131.29801451830267\n",
      "Average value: -585.9028999276866 for episode: 62\n",
      "12800 -265.2837480871287\n",
      "Average value: -569.8719423356587 for episode: 63\n",
      "13000 -378.7325685989451\n",
      "Average value: -560.314973648823 for episode: 64\n",
      "13200 -137.09122667174847\n",
      "Average value: -539.1537862999692 for episode: 65\n",
      "13400 -257.7645162631624\n",
      "Average value: -525.0843227981288 for episode: 66\n",
      "13600 -1224.6406457019582\n",
      "Average value: -560.0621389433203 for episode: 67\n",
      "13800 -127.31005645183959\n",
      "Average value: -538.4245348187462 for episode: 68\n",
      "14000 -1144.8845103883493\n",
      "Average value: -568.7475335972264 for episode: 69\n",
      "14200 -2.0452707020454186\n",
      "Average value: -540.4124204524672 for episode: 70\n",
      "14400 -137.73628642907525\n",
      "Average value: -520.2786137512977 for episode: 71\n",
      "14600 -1082.1798813111357\n",
      "Average value: -548.3736771292896 for episode: 72\n",
      "14800 -350.7224484230837\n",
      "Average value: -538.4911156939793 for episode: 73\n",
      "15000 -131.95888683790352\n",
      "Average value: -518.1645042511755 for episode: 74\n",
      "15200 -278.54600111573444\n",
      "Average value: -506.18357909440334 for episode: 75\n",
      "15400 -247.27785748066503\n",
      "Average value: -493.2382930137164 for episode: 76\n",
      "15600 -137.3927324283429\n",
      "Average value: -475.4460149844477 for episode: 77\n",
      "15800 -268.06984947164125\n",
      "Average value: -465.07720670880735 for episode: 78\n",
      "16000 -1194.1980453838432\n",
      "Average value: -501.53324864255916 for episode: 79\n",
      "16200 -129.77137869254616\n",
      "Average value: -482.9451551450585 for episode: 80\n",
      "16400 -3.611180053233292\n",
      "Average value: -458.9784563904672 for episode: 81\n",
      "16600 -403.38737693108783\n",
      "Average value: -456.19890241749823 for episode: 82\n",
      "16800 -1229.460720347109\n",
      "Average value: -494.86199331397876 for episode: 83\n",
      "17000 -245.56517441897734\n",
      "Average value: -482.3971523692287 for episode: 84\n",
      "17200 -260.77301651901763\n",
      "Average value: -471.3159455767181 for episode: 85\n",
      "17400 -135.40633777730338\n",
      "Average value: -454.52046518674734 for episode: 86\n",
      "17600 -128.9215515086617\n",
      "Average value: -438.24051950284303 for episode: 87\n",
      "17800 -6.905637042303703\n",
      "Average value: -416.67377537981605 for episode: 88\n",
      "18000 -267.57232155241906\n",
      "Average value: -409.21870268844617 for episode: 89\n",
      "18200 -1078.4389351384066\n",
      "Average value: -442.6797143109442 for episode: 90\n",
      "18400 -329.8275226213929\n",
      "Average value: -437.0371047264666 for episode: 91\n",
      "18600 -135.18742166770716\n",
      "Average value: -421.94462057352865 for episode: 92\n",
      "18800 -142.31365936951806\n",
      "Average value: -407.9630725133281 for episode: 93\n",
      "19000 -266.66811393557305\n",
      "Average value: -400.89832458444033 for episode: 94\n",
      "19200 -138.93004794534127\n",
      "Average value: -387.79991075248535 for episode: 95\n",
      "19400 -285.546144915953\n",
      "Average value: -382.68722246065875 for episode: 96\n",
      "19600 -135.93865502145223\n",
      "Average value: -370.34979408869845 for episode: 97\n",
      "19800 -136.55128253630514\n",
      "Average value: -358.6598685110788 for episode: 98\n",
      "20000 -127.67206355240558\n",
      "Average value: -347.1104782631451 for episode: 99\n",
      "20200 -269.31606883929953\n",
      "Average value: -343.2207577919528 for episode: 100\n",
      "20400 -14.319773178855856\n",
      "Average value: -326.77570856129796 for episode: 101\n",
      "20600 -140.7454789945031\n",
      "Average value: -317.4741970829582 for episode: 102\n",
      "20800 -139.98063274691998\n",
      "Average value: -308.59951886615625 for episode: 103\n",
      "21000 -271.05516935143424\n",
      "Average value: -306.7223013904201 for episode: 104\n",
      "21200 -142.87362488060114\n",
      "Average value: -298.52986756492913 for episode: 105\n",
      "21400 -134.67331444082015\n",
      "Average value: -290.3370399087237 for episode: 106\n",
      "21600 -138.39008859594816\n",
      "Average value: -282.7396923430849 for episode: 107\n",
      "21800 -259.69887736153265\n",
      "Average value: -281.5876515940073 for episode: 108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000 -145.37927429537072\n",
      "Average value: -274.7772327290754 for episode: 109\n",
      "22200 -142.5280429987009\n",
      "Average value: -268.16477324255663 for episode: 110\n",
      "22400 -248.98999306630498\n",
      "Average value: -267.20603423374405 for episode: 111\n",
      "22600 -350.26429178896257\n",
      "Average value: -271.35894711150496 for episode: 112\n",
      "22800 -139.82164767402728\n",
      "Average value: -264.7820821396311 for episode: 113\n",
      "23000 -20.11867468122012\n",
      "Average value: -252.54891176671055 for episode: 114\n",
      "23200 -365.58960359612246\n",
      "Average value: -258.2009463581811 for episode: 115\n",
      "23400 -140.16090632626452\n",
      "Average value: -252.29894435658528 for episode: 116\n",
      "23600 -142.11691287026144\n",
      "Average value: -246.7898427822691 for episode: 117\n",
      "23800 -15.86541186106106\n",
      "Average value: -235.24362123620867 for episode: 118\n",
      "24000 -140.71763608398632\n",
      "Average value: -230.51732197859752 for episode: 119\n",
      "24200 -138.08582073350925\n",
      "Average value: -225.89574691634309 for episode: 120\n",
      "24400 -137.62459039950366\n",
      "Average value: -221.48218909050112 for episode: 121\n",
      "24600 -259.47196901793325\n",
      "Average value: -223.38167808687274 for episode: 122\n",
      "24800 -133.1700130632179\n",
      "Average value: -218.87109483569 for episode: 123\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-5f5d0b6d8ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mstep_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-a4474616c59a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# sample from Replay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mbatch_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_next_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplayBuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# update critic (create target for Q function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ceabd25f6d85>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mbatch_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_experience\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mbatch_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_experience\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mbatch_reward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_experience\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mbatch_next_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_experience\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'next_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mbatch_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_experience\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# env = NormalizeAction(env) # remap action values for the environment\n",
    "avg_val = 0\n",
    "\n",
    "#for plotting\n",
    "running_rewards_ddpg = []\n",
    "step_list_ddpg = []\n",
    "step_counter = 0\n",
    "\n",
    "# set term_condition for early stopping according to environment being used\n",
    "term_condition = -150 # Pendulum\n",
    "\n",
    "for itr in range(NUM_EPISODES):\n",
    "    state = env.reset() # get initial state\n",
    "    ddpg.noise.reset()\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    animate_this_episode = (itr % animate_interval == 0) and VISUALIZE\n",
    "\n",
    "    while True:\n",
    "        ddpg.noise.reset()\n",
    "\n",
    "        if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "\n",
    "        # use actor to get action, add ddpg.noise.step() to action\n",
    "        ddpg.actor.eval()\n",
    "        state = state.reshape(1,-1)\n",
    "        noise = ddpg.noise.sample()\n",
    "        action = to_numpy(ddpg.actor(to_tensor(state))).reshape(-1,)+noise\n",
    "        \n",
    "        # remember to put NN in eval mode while testing (to deal with BatchNorm layers) and put it back \n",
    "        # to train mode after you're done getting the action\n",
    "                \n",
    "        # step action, get next state, reward, done (keep track of total_reward)\n",
    "        # populate ddpg.replayBuffer\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        ddpg.replayBuffer.add_experience(state.ravel(), action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        \n",
    "        ddpg.train()\n",
    "        step_counter += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "        \n",
    "    \n",
    "    print(step_counter, total_reward)\n",
    "\n",
    "    if avg_val > term_condition and itr > 10:\n",
    "        break\n",
    "\n",
    "    running_rewards_ddpg.append(total_reward) # return of this episode\n",
    "    step_list_ddpg.append(step_counter)\n",
    "\n",
    "    avg_val = avg_val * 0.95 + 0.05*running_rewards_ddpg[-1]\n",
    "    print(\"Average value: {} for episode: {}\".format(avg_val,itr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot rewards over multiple training runs \n",
    "This is provided to generate and plot results for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "plt.figure()\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg),20)\n",
    "step_list_ddpg = np.array(step_list_ddpg)\n",
    "\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "In this section you will implement REINFORCE, with modifications for batch training. It will be for use on both discrete and continous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Parametrization\n",
    "\n",
    "Define a MLP which outputs a distribution over the action preferences given input state. For the discrete case, the MLP outputs the likelihood of each action (softmax) while for the continuous case, the output is the mean and standard deviation parametrizing the normal distribution from which the action is sampled.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Policy parametrizing model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 1 or 2 hidden layers with a small number of units per layer (similar to DQN)\n",
    "# use ReLU for hidden layer activations\n",
    "# softmax as activation for output if discrete actions, linear for continuous control\n",
    "# for the continuous case, output_dim=2*act_dim (each act_dim gets a mean and std_dev)\n",
    "\n",
    "class mlp(nn.Module):\n",
    "    \n",
    "    def __init__(self, discrete, input_size, output_size, learning_rate = 1e-3):\n",
    "        super(mlp, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "\n",
    "        # Continuous vs Discrete\n",
    "        if discrete:\n",
    "            self.fc3 = nn.Linear(128, output_size)\n",
    "        else:\n",
    "            self.fc3 = nn.Linear(128, 2*output_size)\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = self.learning_rate)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # Layer 1\n",
    "        x = self.fc1(inp)\n",
    "        x = F.relu(x)\n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        # Output layer - Continuous vs Discrete\n",
    "        if discrete:\n",
    "            out = F.softmax(self.fc3(x), dim = -1)    ## Probabilities of categorical actions\n",
    "        else:\n",
    "            out = self.fc3(x)                 ## Mean and variance of actions [ mu1, mu2,......, var1, var2,....]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that samples an action from the policy distribtion parameters obtained as output of the MLP. The function should return the action and the log-probability (log_odds) of taking that action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(logit, discrete):\n",
    "    # logit is the output of the softmax/linear layer\n",
    "    # discrete is a flag for the environment type\n",
    "    # Hint: use Categorical and Normal from torch.distributions to sample action and get the log-probability\n",
    "    # Note that log_probability in this case translates to ln(\\pi(a|s)) \n",
    "\n",
    "    n_actions = logit.shape[0]//2\n",
    "    # Discrete action space: categorical distribution\n",
    "    if discrete:\n",
    "        dist = torch.distributions.Categorical(probs = logit)\n",
    "    # Continuous action space: Normal distribution\n",
    "    else:\n",
    "        means     = x[:n_actions]\n",
    "        # predicted variance are in range 1. Rescale to the action-space\n",
    "        variances = F.sigmoid(x[n_actions:]) * to_tensor(env.action_space.high - env.action_space.low)\n",
    "        dist      = torch.distributions.Normal(means, variances)\n",
    "    \n",
    "    sampled_action = dist.sample()\n",
    "    log_prob       = dist.log_prob(sampled_action) \n",
    "    return sampled_action.cpu().data.numpy()[0], log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function update_policy that defines the loss function and updates the MLP according to the REINFORCE update rule (ref. slide 24 of Lec 7 or page 330 of Sutton and Barto (2018)). The update algorithm to be used below is slightly different: instead of updating the network at every time-step, we take the gradient of the loss averaged over a batch of timesteps (this is to make SGD more stable). We also use a baseline to reduce variance. \n",
    "\n",
    "The discount factor is set as 1 here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(paths, net):\n",
    "    # paths: a list of paths (complete episodes, used to calculate return at each time step)\n",
    "    # net: MLP object\n",
    "    \n",
    "    num_paths = len(paths)\n",
    "    losses = to_tensor(np.zeros(num_paths))\n",
    "    \n",
    "    for path_idx,path in enumerate(paths):\n",
    "        \n",
    "         # rew_cums should record return at each time step for each path \n",
    "        len_path = path['length']\n",
    "        rew_cums = np.zeros(path['log_odds'].shape)\n",
    "        # For the current path, move backward to calculate cumulative rewards\n",
    "        rew_cums[-1] = path['reward'][-1]\n",
    "        for i in range(len_path-2,-1,-1):\n",
    "            rew_cums[i] = rew_cums[i+1] + path['reward'][i]\n",
    "            # log_odds should record log_odds obtained at each timestep of path\n",
    "            # calculated as \"reward to go\"\n",
    "        \n",
    "        rew_cums = (rew_cums - rew_cums.mean()) / (rew_cums.std() + 1e-5) # create baseline\n",
    "        # Loss = Rewards, weighted by grad log probability of action\n",
    "    # make log_odds, rew_cums each a vector\n",
    "    losses[path_idx] = -( to_tensor(rew_cums, requires_grad = False) * path['log_odds']).sum() # Negative for gradient ascent\n",
    "    \n",
    "    # calculate policy loss and average over paths\n",
    "    loss = losses.mean()    \n",
    "    # take optimizer step\n",
    "    net.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    net.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment and instantiate objects. Your algorithm is to be tested on one discrete and two continuous environments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "\nMissing path to your environment variable. \nCurrent values LD_LIBRARY_PATH=/datasets/home/64/264/arv018/.mujoco/*/bin:/usr/lib/x86_64-linux-gnu/mesa:/usr/local/cuda-8.0/targets/x86_64-linux/lib:/usr/local/cuda-8.0/lib64:/usr/lib64/nvidia:/usr/local/cuda/extras/CUPTI/lib64\nPlease add following line to .bashrc:\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/datasets/home/64/264/arv018/.mujoco/mjpro150/bin",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-6dd6fcb289f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Make the gym environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mvisualize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0manimate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mentry_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEntryPoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, require, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2407\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2408\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2412\u001b[0m         \u001b[0mResolve\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentry\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mits\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2413\u001b[0m         \"\"\"\n\u001b[0;32m-> 2414\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__name__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/gym/envs/mujoco/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMujocoEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# ^^^^^ so that user gets the correct error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# message if mujoco is not installed correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAntEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmujoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf_cheetah\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHalfCheetahEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/mujoco_py/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcymj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_mujoco_warnings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMujocoException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerated\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmjviewer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMjViewer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMjViewerBasic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmujoco_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/mujoco_py/builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0mmjpro_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscover_mujoco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m \u001b[0mcymj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cython_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmjpro_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/mujoco_py/builder.py\u001b[0m in \u001b[0;36mload_cython_ext\u001b[0;34m(mjpro_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mBuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMacExtensionBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'linux'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0m_ensure_set_env_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LD_LIBRARY_PATH\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mBuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinuxCPUExtensionBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"win\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/mujoco_py/builder.py\u001b[0m in \u001b[0;36m_ensure_set_env_var\u001b[0;34m(var_name, lib_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m                         \u001b[0;34m\"Please add following line to .bashrc:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                         \"export %s=$%s:%s\" % (var_name, os.environ.get(var_name, \"\"),\n\u001b[0;32m---> 79\u001b[0;31m                                               var_name, var_name, lib_path))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dynamic_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: \nMissing path to your environment variable. \nCurrent values LD_LIBRARY_PATH=/datasets/home/64/264/arv018/.mujoco/*/bin:/usr/lib/x86_64-linux-gnu/mesa:/usr/local/cuda-8.0/targets/x86_64-linux/lib:/usr/local/cuda-8.0/lib64:/usr/lib64/nvidia:/usr/local/cuda/extras/CUPTI/lib64\nPlease add following line to .bashrc:\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/datasets/home/64/264/arv018/.mujoco/mjpro150/bin"
     ]
    }
   ],
   "source": [
    "# Select Environment\n",
    "min_timesteps_per_batch = 2000  # sets the batch size for updating network\n",
    "\n",
    "#discrete environment:\n",
    "# env_name='CartPole-v0'\n",
    "\n",
    "#continous environments:\n",
    "env_name='InvertedPendulum-v2'\n",
    "#env_name = 'HalfCheetah-v2'\n",
    "\n",
    "# env_name='InvertedPendulum-v2'\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make(env_name)\n",
    "visualize = False\n",
    "animate=visualize\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "max_path_length=None\n",
    "\n",
    "\n",
    "# Set random seeds\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Saving parameters\n",
    "logdir='./REINFORCE/'\n",
    "\n",
    "\n",
    "if visualize:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%animate_interval==0)\n",
    "env._max_episodes_steps = min_timesteps_per_batch\n",
    "\n",
    "\n",
    "# Is this env continuous, or discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "# Get observation and action space dimensions\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "# Maximum length for episodes\n",
    "max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "# Make network object (remember to pass in appropriate flags for the type of action space in use)\n",
    "net = mlp(discrete = discrete, input_size = obs_dim, output_size = act_dim, learning_rate = learning_rate)\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run REINFORCE\n",
    "\n",
    "Run REINFORCE for CartPole, InvertedPendulum, and HalfCheetah. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: 56.0\n",
      "Average reward: 49.304664687499994\n",
      "Average reward: 43.97434529001689\n",
      "Average reward: 40.143486374457964\n",
      "Average reward: 37.15784920884656\n",
      "Average reward: 33.704665083804905\n",
      "Average reward: 32.56083640917007\n",
      "Average reward: 33.19837952247173\n",
      "Average reward: 36.11253166787897\n",
      "Average reward: 35.75400360946982\n",
      "Average reward: 40.188326432313936\n",
      "Average reward: 43.9377465283519\n",
      "Average reward: 51.3746194503455\n",
      "Average reward: 64.63686932699406\n",
      "Average reward: 79.02498390740645\n",
      "Average reward: 79.92501957129537\n",
      "Average reward: 83.79864719858276\n",
      "Average reward: 89.23769266555108\n",
      "Average reward: 91.58648455358697\n",
      "Average reward: 96.99456369270376\n",
      "Average reward: 113.23802317654375\n",
      "Average reward: 130.74751998419273\n",
      "Average reward: 135.9574948391686\n",
      "Average reward: 136.5021684418033\n",
      "Average reward: 126.11513086768144\n",
      "Average reward: 120.2257238832297\n",
      "Average reward: 107.31495552548161\n",
      "Average reward: 101.44229533177793\n",
      "Average reward: 104.36450063397498\n",
      "Average reward: 102.2643098922765\n",
      "Average reward: 110.0289701437362\n",
      "Average reward: 114.1587096699797\n",
      "Average reward: 117.71036745472719\n",
      "Average reward: 123.25871473258826\n",
      "Average reward: 130.71459384082718\n",
      "Average reward: 126.98499940458696\n",
      "Average reward: 133.1756906377182\n",
      "Average reward: 143.27752294136354\n",
      "Average reward: 155.172546336746\n",
      "Average reward: 165.31337087870946\n",
      "Average reward: 173.160147599813\n",
      "Average reward: 178.3879963474216\n",
      "Average reward: 183.27704355245442\n",
      "Average reward: 186.6100950822465\n",
      "Average reward: 181.8858396322048\n",
      "Average reward: 173.46277144608203\n",
      "Average reward: 163.31956497339752\n",
      "Average reward: 153.78686359720763\n",
      "Average reward: 142.66969473943192\n",
      "Average reward: 135.38779389831643\n",
      "Average reward: 131.2128184636961\n",
      "Average reward: 130.75998174535607\n",
      "Average reward: 130.53564688740448\n",
      "Average reward: 138.37618897570474\n",
      "Average reward: 138.71697973329793\n",
      "Average reward: 142.62592931269975\n",
      "Average reward: 154.70111185788952\n",
      "Average reward: 163.62547136569012\n",
      "Average reward: 171.85408313222305\n",
      "Average reward: 178.2212260592545\n",
      "Average reward: 183.14799988252935\n",
      "Average reward: 186.9602435503534\n",
      "Average reward: 189.91008502962075\n",
      "Average reward: 192.1926161349246\n",
      "Average reward: 193.95879519345954\n",
      "Average reward: 195.32543088116557\n",
      "Average reward: 196.38290752481967\n",
      "Average reward: 196.63116279353073\n",
      "Average reward: 189.0370233005933\n",
      "Average reward: 176.7557738617424\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-39dbb61a5cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# get parametrized policy distribution from net using current state ob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mact_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;31m# sample action and get log-probability (log_odds) from distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_odd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscrete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/conda-envs/gym/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iter = 1000 \n",
    "avg_reward = 0\n",
    "avg_rewards = []\n",
    "step_list_reinforce = []\n",
    "total_steps = 0\n",
    "episodes = 0\n",
    "animate_this_episode = False\n",
    "for itr in range(n_iter): # loop for number of optimization steps\n",
    "    paths = []\n",
    "    steps = 0\n",
    "    \n",
    "    while True: # loop to get enough timesteps in this batch --> if episode ends this loop will restart till steps reaches limit\n",
    "        ob = env.reset()\n",
    "        obs, acs, rews, log_odds = [], [], [], [] \n",
    "       \n",
    "        while True: # loop for episode inside batch\n",
    "            if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "            \n",
    "            # get parametrized policy distribution from net using current state ob\n",
    "            act_dist = net(to_tensor(ob))\n",
    "            # sample action and get log-probability (log_odds) from distribution\n",
    "            action, log_odd = sample_action(act_dist, discrete = discrete)\n",
    "            # step environment, record reward, next state\n",
    "            next_ob,reward, done, _ = env.step(action)\n",
    "            # append to obs, acs, rewards, log_odds\n",
    "            obs.append(ob)\n",
    "            acs.append(action)\n",
    "            rews.append(reward)\n",
    "            log_odds.append(log_odd)\n",
    "            \n",
    "            # if done, restart episode till min_timesteps_per_batch is reached\n",
    "            steps += 1\n",
    "            if done:\n",
    "                episodes = episodes + 1\n",
    "                break\n",
    "            else:\n",
    "                ob = next_ob\n",
    "                \n",
    "        path = {\"observation\" : obs, \n",
    "                \"reward\" : np.array(rews), \n",
    "                \"action\" : (acs),\n",
    "                \"log_odds\" : torch.stack(log_odds).squeeze(),\n",
    "                \"length\"   : len(obs)\n",
    "               }    \n",
    "        \n",
    "        paths.append(path)\n",
    "        \n",
    "        if steps > min_timesteps_per_batch:\n",
    "            break \n",
    "#     break\n",
    "    update_policy(paths, net)  # use all complete episodes (a batch of timesteps) recorded in this itr to update net\n",
    "    \n",
    "    if itr == 0:\n",
    "        avg_reward = path['reward'].sum()\n",
    "    else:\n",
    "        avg_reward = avg_reward * 0.95 + 0.05 * path['reward'].sum()\n",
    "    \n",
    "    if avg_reward > 300:\n",
    "        break\n",
    "    \n",
    "    total_steps += steps\n",
    "    avg_rewards.append(avg_reward)\n",
    "    step_list_reinforce.append(total_steps)\n",
    "#     if itr % logging_interval == 0:\n",
    "    if itr % 5 == 0:\n",
    "        print('Average reward: {}'.format(avg_reward))\n",
    "   \n",
    "      \n",
    "env.close()\n",
    "\n",
    "plt.plot(avg_rewards)\n",
    "plt.title('Training reward for <env> over multiple runs ')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS (15% extra)\n",
    "\n",
    "Compare average returns for CartPole (discrete action space) when using REINFORCE and DQN. Since in REINFORCE we update the network after a set number of steps instead of after every episode, plot the average rewards as a function of steps rather than episodes for both DQN and REINFORCE. You will need to make minor edits to your DQN code from the previous assignment to record average returns as a function of time_steps.\n",
    "\n",
    "Similarly, compare REINFORCE with DDPG on InvertedPendulum and HalfCheetah using steps for the x-axis.\n",
    "\n",
    "You may use the example code provided below as a reference for the graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # import your DQN and format your average returns as defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg),20)\n",
    "plt.plot(step_list_ddpg, out) # or plt.plot(step_list_DQN, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.legend(['DDPG', 'REINFORCE']) #or plt.legend(['DQN', 'REINFORCE'])\n",
    "plt.plot(step_list_reinforce, avg_rewards)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
